{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sckvDxDNt8N9"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "import numpy as np\r\n",
        "import spacy\r\n",
        "import random\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crFX4oVjuZUQ",
        "outputId": "163653ba-0dfc-48b0-e553-c33dbca9bd9d"
      },
      "source": [
        "!python -m spacy download en_core_web_sm de_core_news_sm\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: de_core_news_sm in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (1.19.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (51.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm) (3.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm) (3.4.0)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FOLMOkEuS9Z"
      },
      "source": [
        "spacy_ger = spacy.load('de_core_news_sm')\r\n",
        "spacy_eng = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3_Z_vNPuxen"
      },
      "source": [
        "def tokenizer_ger(text):\r\n",
        "    return [tok.text for tok in spacy_ger.tokenizer(text)]\r\n",
        "def tokenizer_eng(text):\r\n",
        "    return [tok.text for tok in spacy_eng.tokenizer(text)]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7e2cEYuvwQf"
      },
      "source": [
        "german = Field(tokenize=tokenizer_ger, lower=True, init_token='<sos>', eos_token='<eos>')\r\n",
        "english = Field(tokenize=tokenizer_eng, lower=True, init_token='<sos>', eos_token='<eos>')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjqiwMKnwRse"
      },
      "source": [
        "train_data, validation_data, test_data = Multi30k.splits(exts=('.de', '.en'),\r\n",
        "                                                         fields = (german, english))\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stkkUmLVxyvD"
      },
      "source": [
        "german.build_vocab(train_data, max_size=10000, min_freq=2)\r\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H48nys1x8Ht"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, input_size, embedding_size, hidden_szie, num_layers, p):\r\n",
        "        super(Encoder, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(p)\r\n",
        "        self.embedding =  nn.Embedding(input_size, embedding_size)\r\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        embedding = self.dropout(self.embedding(x))\r\n",
        "        # shape of X is (seq leng, N), N is batch size\r\n",
        "        # embedding shape == (seq_length, N, embedding_size(lets say 300))\r\n",
        "        output, (hidden, cell) = self.rnn(embedding)\r\n",
        "        return hidden, cell\r\n",
        "\r\n",
        "\r\n",
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\r\n",
        "        super(Decoder, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(p)\r\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\r\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\r\n",
        "        #hidden size of encoder and decoder are the same\r\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\r\n",
        "\r\n",
        "    def forward(self, x, hidden, cell):\r\n",
        "        # shape of x is (N), we wnat  (1, N), in encoder we are sneding a sentence, whereas in decoder we take a single word input from the predicted output and feed it as inoput\r\n",
        "        x = x.unsqueeze(0)\r\n",
        "        embedding = self.dropout(self.embedding(x))\r\n",
        "        # embedding shape is (1, N, embedding_size)\r\n",
        "        output, (hidden, cell) = self.rnn(embedding, (hidden, cell))\r\n",
        "        # shape of output is 1, N, hidden_size\r\n",
        "        predictions = self.fc(output)\r\n",
        "        # shape of preds is (1, N, length of vocab)\r\n",
        "        predictions = predictions.squeeze(0)\r\n",
        "        return predictions, hidden, cell\r\n",
        "\r\n",
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder):\r\n",
        "        super(Seq2Seq, self).__init__()\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "\r\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\r\n",
        "        # teacher force ratio 50% of the time we send predicted outputs as inputs in the decoder and 50% of the time we send the actual values to the decoder inputs\r\n",
        "        batch_size = source.shape[1] #()\r\n",
        "        target_len = target.shape[0]\r\n",
        "        target_vocab_size = len(english.vocab)\r\n",
        "\r\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(\"cuda\")\r\n",
        "        hidden, cell = self.encoder(source)\r\n",
        "\r\n",
        "        x = target[0] # get start token\r\n",
        "        for t in range(1, target_len):\r\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\r\n",
        "            # output has the final predcitons, outputs contain the whole sentences\r\n",
        "            outputs[t] = output\r\n",
        "\r\n",
        "            best_guess = output.argmax(1)\r\n",
        "\r\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\r\n",
        "\r\n",
        "        return outputs"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFtcg1JqaQlq"
      },
      "source": [
        "# Training"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0TDHXqrayy7"
      },
      "source": [
        "num_epochs = 20\r\n",
        "learning_rate = 0.001\r\n",
        "batch_size = 64\r\n",
        "load_model = False\r\n",
        "device = torch.device('cuda')\r\n",
        "input_size_encoder = len(german.vocab)\r\n",
        "output_size_decoder = len(english.vocab)\r\n",
        "output_size = len(english.vocab)\r\n",
        "encoder_embedding_size = 300\r\n",
        "decoder_embedding_size = 300\r\n",
        "hidden_size = 1024 # used in seq2seq paper\r\n",
        "\r\n",
        "num_layers = 2\r\n",
        "enc_dropout = 0.5\r\n",
        "dec_dropout = 0.5\r\n",
        "\r\n",
        "# Tensorboard\r\n",
        "writer = SummaryWriter(f'runs/loss_plot')\r\n",
        "step = 0\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, validation_data, test_data), batch_size=batch_size, sort_within_batch=True, sort_key = lambda x: len(x.src), device=device)\r\n",
        "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout).to(device)\r\n",
        "decoder_net = Decoder(output_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout).to(device)\r\n",
        "\r\n",
        "model = Seq2Seq(encoder_net, decoder_net).to(device)\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\r\n",
        "\r\n",
        "pad_idx = english.vocab.stoi['<pad>']\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\r\n",
        "\r\n",
        "if load_model:\r\n",
        "    load_checkpoint(torch.load('my_checkpoint.pth.ptar'), model, optimizer)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eADrsvRBkrRz",
        "outputId": "e7f775ad-0647-49fa-f1d9-d4419cc8f490"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.19.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (0.1.94)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI3EZkvya0LO"
      },
      "source": [
        "import torch\r\n",
        "import spacy\r\n",
        "from torchtext.data.metrics import bleu_score\r\n",
        "import sys\r\n",
        "\r\n",
        "\r\n",
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\r\n",
        "    # print(sentence)\r\n",
        "\r\n",
        "    # sys.exit()\r\n",
        "\r\n",
        "    # Load german tokenizer\r\n",
        "    spacy_ger = spacy.load(\"de_core_news_sm\")\r\n",
        "\r\n",
        "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\r\n",
        "    if type(sentence) == str:\r\n",
        "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\r\n",
        "    else:\r\n",
        "        tokens = [token.lower() for token in sentence]\r\n",
        "\r\n",
        "    # print(tokens)\r\n",
        "\r\n",
        "    # sys.exit()\r\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\r\n",
        "    tokens.insert(0, german.init_token)\r\n",
        "    tokens.append(german.eos_token)\r\n",
        "\r\n",
        "    # Go through each german token and convert to an index\r\n",
        "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\r\n",
        "\r\n",
        "    # Convert to Tensor\r\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\r\n",
        "\r\n",
        "    # Build encoder hidden, cell state\r\n",
        "    with torch.no_grad():\r\n",
        "        hidden, cell = model.encoder(sentence_tensor)\r\n",
        "\r\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\r\n",
        "\r\n",
        "    for _ in range(max_length):\r\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\r\n",
        "            best_guess = output.argmax(1).item()\r\n",
        "\r\n",
        "        outputs.append(best_guess)\r\n",
        "\r\n",
        "        # Model predicts it's the end of the sentence\r\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\r\n",
        "            break\r\n",
        "\r\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\r\n",
        "\r\n",
        "    # remove start token\r\n",
        "    return translated_sentence[1:]\r\n",
        "\r\n",
        "\r\n",
        "def bleu(data, model, german, english, device):\r\n",
        "    targets = []\r\n",
        "    outputs = []\r\n",
        "\r\n",
        "    for example in data:\r\n",
        "        src = vars(example)[\"src\"]\r\n",
        "        trg = vars(example)[\"trg\"]\r\n",
        "\r\n",
        "        prediction = translate_sentence(model, src, german, english, device)\r\n",
        "        prediction = prediction[:-1]  # remove <eos> token\r\n",
        "\r\n",
        "        targets.append([trg])\r\n",
        "        outputs.append(prediction)\r\n",
        "\r\n",
        "    return bleu_score(outputs, targets)\r\n",
        "\r\n",
        "\r\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\r\n",
        "    print(\"=> Saving checkpoint\")\r\n",
        "    torch.save(state, filename)\r\n",
        "\r\n",
        "\r\n",
        "def load_checkpoint(checkpoint, model, optimizer):\r\n",
        "    print(\"=> Loading checkpoint\")\r\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\r\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6WHCEc_a0Ig",
        "outputId": "906f2cc7-3345-48af-8389-600b2f2c329d"
      },
      "source": [
        "sentence = \"ein boot mit mehreren mÃ¤nnern darauf wird von einem groÃŸen pferdegespann ans ufer gezogen.\"\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    print(f'Epoch [{epoch}/{num_epochs}]')\r\n",
        "\r\n",
        "    checkpoint = {'state_dict':model.state_dict(), 'optimizer':optimizer.state_dict()}\r\n",
        "    save_checkpoint(checkpoint)\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    translated_sentence = translate_sentence(\r\n",
        "        model, sentence, german, english, device, max_length=50\r\n",
        "    )\r\n",
        "\r\n",
        "    print(f\"Translated example sentence: \\n {translated_sentence}\")\r\n",
        "\r\n",
        "    model.train()\r\n",
        "    for batch_idx, batch in enumerate(train_iterator):\r\n",
        "        inp_data = batch.src.to(device)\r\n",
        "        target = batch.trg.to(device)\r\n",
        "        output = model(inp_data, target)\r\n",
        "\r\n",
        "        # output_shape: (trg_len, batch_size, output_dim )\r\n",
        "\r\n",
        "        output = output[1:].reshape(-1, output.shape[2])\r\n",
        "        target = target[1:].reshape(-1)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss = criterion(output, target)\r\n",
        "        loss.backward()\r\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1)\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        writer.add_scalar('Trainig Loss', loss, global_step=step)\r\n",
        "        step += 1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'being', 'pulled', 'by', 'a', 'large', ',', 'by', 'a', 'large', '.', '<eos>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'being', 'pulled', 'by', 'a', 'boat', 'full', 'of', 'by', 'a', 'large', '.', '<eos>']\n",
            "Epoch [2/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'shore', 'by', 'a', 'large', 'of', 'large', 'horses', '.', '<eos>']\n",
            "Epoch [3/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'boat', ',', 'is', 'being', 'lifted', 'by', 'horses', '.', '<eos>']\n",
            "Epoch [4/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'by', 'a', 'shore', 'of', 'a', 'large', 'ship', '.', '<eos>']\n",
            "Epoch [5/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'pulled', 'by', 'the', 'shore', 'by', 'a', 'large', 'ship', '.', '<eos>']\n",
            "Epoch [6/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'are', 'pulled', 'by', 'shore', 'by', 'several', 'several', 'large', 'horses', '.', '<eos>']\n",
            "Epoch [7/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', ',', 'by', 'a', 'large', 'orange', 'horses', '.', '<eos>']\n",
            "Epoch [8/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', ',', 'large', 'several', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [9/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'shore', 'by', 'a', 'large', 'boat', '.', '<eos>']\n",
            "Epoch [10/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'pulled', 'pulled', 'by', 'shore', 'by', 'a', 'large', 'boat', '.', '<eos>']\n",
            "Epoch [11/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'by', 'shore', 'by', 'a', 'large', 'boat', '.', '<eos>']\n",
            "Epoch [12/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'by', 'shore', 'by', 'a', 'large', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [13/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'are', 'pulled', 'by', 'a', 'large', 'team', 'of', 'of', 'police', 'at', 'horses', '.', '<eos>']\n",
            "Epoch [14/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'by', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [15/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [16/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'pulled', 'pulled', 'by', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [17/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'by', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [18/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'by', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [19/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'by', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMybfkNoa0Fc"
      },
      "source": [
        "score = bleu(test_data, model, german, english, device)\r\n",
        "print(f\"Bleu score {score*100:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90h6kBExaz7V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFJvy5lwaz4a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}