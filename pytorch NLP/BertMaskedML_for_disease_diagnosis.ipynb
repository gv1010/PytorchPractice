{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertMasedML for disease diagnosis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c5ea3259de5a48f2b9c7332695a87c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_799ebe0592e74dbaa8ed93189aaf8f1c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_60c49b5e68384540bfba3bf53010fdf4",
              "IPY_MODEL_d69cd1321d56406db08340fb51d57d0f"
            ]
          }
        },
        "799ebe0592e74dbaa8ed93189aaf8f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60c49b5e68384540bfba3bf53010fdf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_de16fc196e1845ed9a9cba88b54646d9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7a8e7460c8a4df7ac6c92c1f1812208"
          }
        },
        "d69cd1321d56406db08340fb51d57d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_37ce94b66ffe4353b54db723c700640b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 3.76MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb3bcb1f06a84611bc9eb5b41580dff6"
          }
        },
        "de16fc196e1845ed9a9cba88b54646d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7a8e7460c8a4df7ac6c92c1f1812208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37ce94b66ffe4353b54db723c700640b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb3bcb1f06a84611bc9eb5b41580dff6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsFk1V56Bb3i",
        "outputId": "058b435c-1ea7-4ae5-dedb-796b49a924eb"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 12.6MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=b0488d1588e0350b0e3084545b4e1edba8933195bb9665f03f4fad76eb9d633c\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sERwC5M1BxtJ"
      },
      "source": [
        "import torch\r\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c5ea3259de5a48f2b9c7332695a87c7f",
            "799ebe0592e74dbaa8ed93189aaf8f1c",
            "60c49b5e68384540bfba3bf53010fdf4",
            "d69cd1321d56406db08340fb51d57d0f",
            "de16fc196e1845ed9a9cba88b54646d9",
            "d7a8e7460c8a4df7ac6c92c1f1812208",
            "37ce94b66ffe4353b54db723c700640b",
            "eb3bcb1f06a84611bc9eb5b41580dff6"
          ]
        },
        "id": "5XD4ZF8kB3R6",
        "outputId": "d1c10306-481f-4bde-9e5d-cba1864a0951"
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5ea3259de5a48f2b9c7332695a87c7f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCuV5iNWiEjU"
      },
      "source": [
        "# Text with symptoms in it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLqRqcTJKDbV"
      },
      "source": [
        "text  = \"Pain or discomfort in one or both arms, the back, neck, jaw or stomach, Shortness of breath with or without chest discomfort, Other signs such as breaking out in a cold sweat, nausea or lightheadedness are symptoms of [MASK] [SEP]\"\r\n",
        "text = \"[MASK] attack is caused by blockage of blood\"\r\n",
        "text = \"[MASK] attack has following symptoms: [MASK] in urine, [MASK] in the lower back.\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3qWHOvmh5XA"
      },
      "source": [
        "#Using a pre-trained bert for maksed LM to predict the disease, given tetx with symptoms in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdTsFS4oKFvF"
      },
      "source": [
        "# Predict tokens\r\n",
        "\r\n",
        "def bertmlm(text):\r\n",
        "  tokenized_text = tokenizer.tokenize(text)\r\n",
        "  #tokenized_text[masked_index] = '[MASK]'\r\n",
        "  mask = []\r\n",
        "  for i, m in enumerate(tokenized_text):\r\n",
        "    if m == '[MASK]':\r\n",
        "      mask.append(i)\r\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\r\n",
        "  segments_ids = [0]*len(indexed_tokens)\r\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\r\n",
        "  segments_tensors = torch.tensor([segments_ids])\r\n",
        "  model_mlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\r\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\r\n",
        "  segments_tensors = torch.tensor([segments_ids])\r\n",
        "\r\n",
        "  tokens_tensor = tokens_tensor.to('cuda')\r\n",
        "  segments_tensors = segments_tensors.to('cuda')\r\n",
        "  model_mlm.to('cuda')\r\n",
        "  model_mlm.eval()\r\n",
        "  with torch.no_grad():\r\n",
        "      outputs = model_mlm(tokens_tensor, token_type_ids=segments_tensors)\r\n",
        "      predictions = outputs[0]\r\n",
        "  print(tokenized_text)\r\n",
        "  for i in mask:\r\n",
        "    predicted_index = torch.argmax(predictions[0, i]).item()\r\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\r\n",
        "    print(predicted_index,predicted_token)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfAGPVo8S21H"
      },
      "source": [
        "# Loading the disease symptoms data,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "zzx8ArhVKIsc",
        "outputId": "ddaebf23-2051-42d0-de18-79cc93eebd6c"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "df = pd.read_csv(\"diseases_texts.csv\")\r\n",
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symptoms</th>\n",
              "      <th>disease</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what are the symptoms of hemorrhagic septicemi...</td>\n",
              "      <td>hemorrhagic septicemia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what are the symptoms of jaundice? s that late...</td>\n",
              "      <td>jaundice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what are the symptoms of drowning? drowning is...</td>\n",
              "      <td>drowning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what are the symptoms of iron-deficiency anemi...</td>\n",
              "      <td>iron-deficiency anemia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what are the symptoms of mixed connective tiss...</td>\n",
              "      <td>mixed connective tissue disease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2049</th>\n",
              "      <td>what are the symptoms of patau syndrome? male ...</td>\n",
              "      <td>patau syndrome</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2050</th>\n",
              "      <td>what are the symptoms of lambert–eaton myasthe...</td>\n",
              "      <td>lambert–eaton myasthenic syndrome</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2051</th>\n",
              "      <td>what are the symptoms of reward system? addict...</td>\n",
              "      <td>reward system</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2052</th>\n",
              "      <td>what are the symptoms of treponema pallidum? t...</td>\n",
              "      <td>treponema pallidum</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2053</th>\n",
              "      <td>what are the symptoms of vipoma? the major cli...</td>\n",
              "      <td>vipoma</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2054 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               symptoms                            disease\n",
              "0     what are the symptoms of hemorrhagic septicemi...             hemorrhagic septicemia\n",
              "1     what are the symptoms of jaundice? s that late...                           jaundice\n",
              "2     what are the symptoms of drowning? drowning is...                           drowning\n",
              "3     what are the symptoms of iron-deficiency anemi...             iron-deficiency anemia\n",
              "4     what are the symptoms of mixed connective tiss...    mixed connective tissue disease\n",
              "...                                                 ...                                ...\n",
              "2049  what are the symptoms of patau syndrome? male ...                     patau syndrome\n",
              "2050  what are the symptoms of lambert–eaton myasthe...  lambert–eaton myasthenic syndrome\n",
              "2051  what are the symptoms of reward system? addict...                      reward system\n",
              "2052  what are the symptoms of treponema pallidum? t...                 treponema pallidum\n",
              "2053  what are the symptoms of vipoma? the major cli...                             vipoma\n",
              "\n",
              "[2054 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0JhqKEGKYJk"
      },
      "source": [
        "data = df[\"symptoms\"].tolist()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAAABhCWrJax"
      },
      "source": [
        ""
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RKTfwmxiPKw"
      },
      "source": [
        "# Preparing the data for fine tuninin the bert for masked model. Adding mask tokens in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrWL8sMvpk92"
      },
      "source": [
        "def get_masked_input_and_labels(encoded_texts):\r\n",
        "    # 15% BERT masking\r\n",
        "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.20\r\n",
        "    # Do not mask special tokens\r\n",
        "    inp_mask[encoded_texts <= 2] = False\r\n",
        "    # Set targets to -1 by default, it means ignore\r\n",
        "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\r\n",
        "    # Set labels for masked tokens\r\n",
        "    labels[inp_mask] = encoded_texts[inp_mask]\r\n",
        "\r\n",
        "    # Prepare input\r\n",
        "    encoded_texts_masked = np.copy(encoded_texts)\r\n",
        "    # Set input to [MASK] which is the last token for the 90% of tokens\r\n",
        "    # This means leaving 10% unchanged\r\n",
        "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\r\n",
        "    encoded_texts_masked[\r\n",
        "        inp_mask_2mask\r\n",
        "    ] = mask_token_id  # mask token is the last in the dict\r\n",
        "\r\n",
        "    # Set 10% to a random token\r\n",
        "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\r\n",
        "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\r\n",
        "        3, mask_token_id, inp_mask_2random.sum()\r\n",
        "    )\r\n",
        "\r\n",
        "    # Prepare sample_weights to pass to .fit() method\r\n",
        "    sample_weights = np.ones(labels.shape)\r\n",
        "    sample_weights[labels == -1] = 0\r\n",
        "\r\n",
        "    # y_labels would be same as encoded_texts i.e input tokens\r\n",
        "    y_labels = np.copy(encoded_texts)\r\n",
        "\r\n",
        "    return torch.tensor(encoded_texts_masked), torch.tensor(y_labels), sample_weights"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FVRYYenib7y"
      },
      "source": [
        "Setting the max token length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gxy77Fovcbe"
      },
      "source": [
        "tokenizer.model_max_length = 128"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2NIshcswG74"
      },
      "source": [
        "#len(encoded['input_ids'][0])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCBQj-3yii7c"
      },
      "source": [
        "Data loader essentials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WUictBXzb8D"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "class Dataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, encodings, labels):\r\n",
        "        self.encodings = encodings\r\n",
        "        self.labels = labels\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\r\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\r\n",
        "        return item\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.labels)\r\n",
        "\r\n",
        "#train_dataset = Dataset(train_encodings, train_labels)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5uJw2PZqLK9"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iV-ua2prmHP"
      },
      "source": [
        "from torch.utils.data import DataLoader\r\n",
        "from transformers import  AdamW\r\n",
        "from tqdm import tqdm\r\n",
        "import time"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD3MLcTA1dw9"
      },
      "source": [
        "device = torch.device('cuda')\r\n",
        "mask_token_id = 103"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NAmTxb6teLs",
        "outputId": "128ade7a-6b7d-4f10-96bc-8407c239d889"
      },
      "source": [
        "encoded = tokenizer(text, padding = True, truncation=True)\r\n",
        "get_masked_input_and_labels(torch.tensor(encoded['input_ids']))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([   64,   103,  2886,  2038,  2206,  8030,  1024,   103,  1999, 17996,\n",
              "          1010,   103,  1999,   103,  2896,  2067,  1012,   102]),\n",
              " tensor([  101,   103,  2886,  2038,  2206,  8030,  1024,   103,  1999, 17996,\n",
              "          1010,   103,  1999,  1996,  2896,  2067,  1012,   102]),\n",
              " array([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "        0.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h6NkjyMtjqs"
      },
      "source": [
        "def train(data):\r\n",
        "  mask_token_id = 103\r\n",
        "  encoded = tokenizer(data, padding=True, truncation=True)\r\n",
        "  encoded['input_ids'], y, w = get_masked_input_and_labels(torch.tensor(encoded['input_ids']))\r\n",
        "\r\n",
        "  train_dataset = Dataset(encoded, y)\r\n",
        "  train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\r\n",
        "  optim = AdamW(model.parameters(), lr=5e-5)\r\n",
        "\r\n",
        "  model.to(device)\r\n",
        "  model.train()\r\n",
        "  for epoch in tqdm(range(5)):\r\n",
        "      start = time.time()\r\n",
        "      for batch in (train_loader):\r\n",
        "          optim.zero_grad()\r\n",
        "          input_ids = batch['input_ids'].to(device)\r\n",
        "          attention_mask = batch['attention_mask'].to(device)\r\n",
        "          labels = batch['labels'].to(device)\r\n",
        "          outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\r\n",
        "          loss = outputs[0]\r\n",
        "          loss.backward()\r\n",
        "          optim.step()\r\n",
        "      print(time.time() - start)\r\n",
        "      print(loss.item())\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WedpMl5qjqrr"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXhs_adFjyEC"
      },
      "source": [
        "# Fine tuning bert for masked model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzvJPhig1b4D",
        "outputId": "1d9654ca-f8f2-4553-b5ef-7b81b1558cf0"
      },
      "source": [
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\r\n",
        "model_fine = train(data)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "\n",
            " 20%|██        | 1/5 [01:01<04:04, 61.06s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "61.040332555770874\n",
            "0.4186389446258545\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 40%|████      | 2/5 [02:04<03:05, 61.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "63.59367656707764\n",
            "0.3388884961605072\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 60%|██████    | 3/5 [03:08<02:04, 62.49s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "64.00246238708496\n",
            "0.16335897147655487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 80%|████████  | 4/5 [04:13<01:03, 63.14s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "64.64537835121155\n",
            "0.1364319920539856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 5/5 [05:18<00:00, 63.64s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "64.7859616279602\n",
            "0.1591934859752655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPMAfTyzCYZD"
      },
      "source": [
        " "
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-5cYPuyChtL"
      },
      "source": [
        "#text  = \"[SEP] Pain or discomfort in one or both arms, the back, neck, jaw or stomach, Shortness of breath with or without chest discomfort, Other signs such as breaking out in a cold sweat, nausea or lightheadedness are symptoms of [MASK]\"\r\n",
        "#text = \"[SEP] I got injured in an accident, I need your [MASK]\"\r\n",
        "#text = \"[SEP] [MASK] cancer have the following symptoms pain or discomfort in one or both arms, the back, neck, jaw or stomach, Shortness of breath with or without chest discomfort \""
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_CZH86IFp5t"
      },
      "source": [
        ""
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xli2FmalFEH6"
      },
      "source": [
        ""
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtRIcZO1j-kS"
      },
      "source": [
        "# Chekcing the performance of the fine tuned model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgmSxg9WChtg"
      },
      "source": [
        "\r\n",
        "def bertfine(model, text):\r\n",
        "  model.eval()\r\n",
        "  tokenizer.model_max_length = 128\r\n",
        "  tokenized_text = tokenizer.encode_plus(text, padding=True, truncation=True)\r\n",
        "\r\n",
        "  # Predict all tokens\r\n",
        "  masked_index = 11\r\n",
        "  #tokenized_text = tokenizer.tokenize(text)\r\n",
        "  #tokenized_text[masked_index] = '[MASK]'\r\n",
        "  mask = []\r\n",
        "  for i, m in enumerate(tokenizer.convert_ids_to_tokens(tokenized_text[\"input_ids\"])):\r\n",
        "    if m == '[MASK]':\r\n",
        "      mask.append(i)\r\n",
        "\r\n",
        "  print(tokenizer.convert_ids_to_tokens(tokenized_text[\"input_ids\"]))\r\n",
        "  #indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text[\"input_ids\"])\r\n",
        "  indexed_tokens = tokenized_text[\"input_ids\"]\r\n",
        "  segments_ids = [0]*len(indexed_tokens)\r\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\r\n",
        "  segments_tensors = torch.tensor([segments_ids])\r\n",
        "  #model = BertForMaskedLM.from_pretrained('bert-base-uncased')\r\n",
        "\r\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\r\n",
        "  segments_tensors = torch.tensor([segments_ids])\r\n",
        "\r\n",
        "  tokens_tensor = tokens_tensor.to('cuda')\r\n",
        "  segments_tensors = segments_tensors.to('cuda')\r\n",
        "\r\n",
        "  with torch.no_grad():\r\n",
        "      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\r\n",
        "      predictions = outputs[0]\r\n",
        "\r\n",
        "  for i in mask:\r\n",
        "    predicted_index = torch.argmax(predictions[0, i]).item()\r\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\r\n",
        "    print(predicted_index,predicted_token)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhST8v3rP8a_"
      },
      "source": [
        "text = \"woman with [MASK] [MASK], signs and symptoms that are related to the infection are mainly:  high fever, usually above 101 °fahrenheit|f / 38 °c  chills  severe abdominal pain and/or cramping /or strong perineum|perineal pressure  beginning miscarriage symptoms (heavy bleeding and or cramping) that suddenly stops and does not resume  prolonged or heavy vaginal bleeding  foul-smelling vaginal discharge backache or heavy back pressure  a cold or urinary tract infection may mimic many of the symptoms.\""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHWpR3XYapif"
      },
      "source": [
        "texts = [\"Blood in the urine, pain while passing urine, pain in the lower back, nausea and vomiting are signs or symptoms of [MASK]\",\r\n",
        "        \"Blood in urine, pain in the left side lower back, nausea and vomiting are symptoms of [MASK] disease\",\r\n",
        "        \"[MASK] pain, swelling of the [MASK], vomiting are symptoms of Appendicites.\",\r\n",
        "        \"[MASK] disease will have following symptoms, blood in urine, pain  in the lower back, [MASK] and vomiting.\"]\r\n",
        "#text = \r\n",
        "#text = \r\n",
        "#text = "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynqPN8wJD2Ig",
        "outputId": "7324329a-e5e4-4ea4-b8a4-dd826b5a3a0f"
      },
      "source": [
        "for text in texts:\r\n",
        "  print(\"Bert\")\r\n",
        "  print(\"=\"*50)\r\n",
        "  bertmlm(text)\r\n",
        "  print()\r\n",
        "  print()\r\n",
        "  print(\"Fine Tuned\")\r\n",
        "  print(\"=\"*50)\r\n",
        "  bertfine(model_fine, text)\r\n",
        "  print()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bert\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['blood', 'in', 'the', 'urine', ',', 'pain', 'while', 'passing', 'urine', ',', 'pain', 'in', 'the', 'lower', 'back', ',', 'nausea', 'and', 'vomiting', 'are', 'signs', 'or', 'symptoms', 'of', '[MASK]']\n",
            "1996 the\n",
            "\n",
            "\n",
            "Fine Tuned\n",
            "==================================================\n",
            "['[CLS]', 'blood', 'in', 'the', 'urine', ',', 'pain', 'while', 'passing', 'urine', ',', 'pain', 'in', 'the', 'lower', 'back', ',', 'nausea', 'and', 'vomiting', 'are', 'signs', 'or', 'symptoms', 'of', '[MASK]', '[SEP]']\n",
            "1012 .\n",
            "\n",
            "Bert\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['blood', 'in', 'urine', ',', 'pain', 'in', 'the', 'left', 'side', 'lower', 'back', ',', 'nausea', 'and', 'vomiting', 'are', 'symptoms', 'of', '[MASK]', 'disease']\n",
            "1996 the\n",
            "\n",
            "\n",
            "Fine Tuned\n",
            "==================================================\n",
            "['[CLS]', 'blood', 'in', 'urine', ',', 'pain', 'in', 'the', 'left', 'side', 'lower', 'back', ',', 'nausea', 'and', 'vomiting', 'are', 'symptoms', 'of', '[MASK]', 'disease', '[SEP]']\n",
            "1996 the\n",
            "\n",
            "Bert\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['[MASK]', 'pain', ',', 'swelling', 'of', 'the', '[MASK]', ',', 'vomiting', 'are', 'symptoms', 'of', 'app', '##end', '##ici', '##tes', '.']\n",
            "1012 .\n",
            "2677 mouth\n",
            "\n",
            "\n",
            "Fine Tuned\n",
            "==================================================\n",
            "['[CLS]', '[MASK]', 'pain', ',', 'swelling', 'of', 'the', '[MASK]', ',', 'vomiting', 'are', 'symptoms', 'of', 'app', '##end', '##ici', '##tes', '.', '[SEP]']\n",
            "21419 abdominal\n",
            "4308 stomach\n",
            "\n",
            "Bert\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['[MASK]', 'disease', 'will', 'have', 'following', 'symptoms', ',', 'blood', 'in', 'urine', ',', 'pain', 'in', 'the', 'lower', 'back', ',', '[MASK]', 'and', 'vomiting', '.']\n",
            "1012 .\n",
            "3255 pain\n",
            "\n",
            "\n",
            "Fine Tuned\n",
            "==================================================\n",
            "['[CLS]', '[MASK]', 'disease', 'will', 'have', 'following', 'symptoms', ',', 'blood', 'in', 'urine', ',', 'pain', 'in', 'the', 'lower', 'back', ',', '[MASK]', 'and', 'vomiting', '.', '[SEP]']\n",
            "1996 the\n",
            "19029 nausea\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Q8TiYFp3-g"
      },
      "source": [
        "# Summary :\r\n",
        "Fine tuned the BERTforMaskedmodel on the data extracted, trained it for nearly 50 epochs, \r\n",
        "The model failed to guess the masked word in a sentence, but when given with the second name of the disease (removed the first name), it would predict that name, eg. if given 1. [MASK] attack has irregular heatbeat, pain in the arm and chest. The finetuned bertformask would give the word [heart] for masked token, but it fails to predict the whole disease name (heart attack)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ISaSKTXl7fz"
      },
      "source": [
        "# END(work in progress)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "DQuRwW3LE9vw",
        "outputId": "3ccfbafc-3b50-46fa-e5eb-fbb1bf2aad9a"
      },
      "source": [
        "data[1]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'what are the symptoms of jaundice? s that later proved to be a manifestation of hemolytic anemia due to g6pd deficiency following vicia faba|fava bean consumption. the main sign of jaundice is a yellowish discoloration of the white area of the eye (sclera) and the skin. choluria|urine is dark in colour. slight increases in serum bilirubin are best detected by examining the sclerae, which have a particular affinity for bilirubin due to their high elastin content. the presence of scleral icterus indicates a serum bilirubin of at least 3 mg/dl. the conjunctiva of the eye are one of the first tissue (biology)|tissues to change color as bilirubin levels rise in jaundice. this is sometimes referred to as \\'\\'scleral icterus\\'\\'. the sclera themselves are not \"icteric\" (stained with bilin (biochemistry)|bile pigment), however, but rather the conjunctival membranes that overlie them. the yellowing of the \"white of the eye\" is thus more properly termed \\'\\'conjunctival icterus\\'\\'. the term \"icterus\" itself is sometimes incorrectly used to refer to jaundice that is noted in the sclera of the eyes; its more common and more correct meaning is entirely synonymous with jaundice, however.   teeth   a yellowish or greenish pigmentation (biliverdin deposition) occurs in the teeth of children with hyperbilirubinemia during calcification, which may be seen in the primary teeth of people with biliary atresia. this is not seen in adults who develop liver disease. disorders associated with an early rise in serum levels of conjugated bilirubin can cause dental hypoplasia. people with parenchymal liver disease who have impaired haemostasis can present bleeding problems if surgery is needed.   complications   hyperbilirubinemia, more precisely hyperbilirubinemia due to the unconjugated fraction, may cause bilirubin to accumulate in the gray matter of the central nervous system, potentially causing irreversible neurological damage leading to a condition known as kernicterus. depending on the level of exposure, the effects range from unnoticeable to severe brain damage and even death. newborns are especially vulnerable to hyperbilirubinemia-induced neurological damage and therefore must be carefully monitored for alterations in their serum bilirubin levels. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUcx2gQXfzjT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jleoU7B2fzf9",
        "outputId": "7900c889-c3a9-448e-9f6f-ef2a50edcf34"
      },
      "source": [
        "import os\r\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\r\n",
        "output_dir = './model_save/'\r\n",
        "\r\n",
        "# Create output directory if needed\r\n",
        "if not os.path.exists(output_dir):\r\n",
        "    os.makedirs(output_dir)\r\n",
        "\r\n",
        "print(\"Saving model to %s\" % output_dir)\r\n",
        "\r\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\r\n",
        "# They can then be reloaded using `from_pretrained()`\r\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\r\n",
        "model_to_save.save_pretrained(output_dir)\r\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/tokenizer_config.json',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/vocab.txt',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7TtbnS6f5gr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBGHcRqikaAP"
      },
      "source": [
        "# Bio BERT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cWn4et0kZ9W"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\r\n",
        "#model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdPO6Pjx0GAH"
      },
      "source": [
        "#tokenizer.tokenize(text)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmM5YnFK0WyS",
        "outputId": "83cd3a35-39d8-4766-cae1-abd5b476816a"
      },
      "source": [
        "tokenizer.max_model_input_sizes"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'TurkuNLP/bert-base-finnish-cased-v1': 512,\n",
              " 'TurkuNLP/bert-base-finnish-uncased-v1': 512,\n",
              " 'bert-base-cased': 512,\n",
              " 'bert-base-cased-finetuned-mrpc': 512,\n",
              " 'bert-base-chinese': 512,\n",
              " 'bert-base-german-cased': 512,\n",
              " 'bert-base-german-dbmdz-cased': 512,\n",
              " 'bert-base-german-dbmdz-uncased': 512,\n",
              " 'bert-base-multilingual-cased': 512,\n",
              " 'bert-base-multilingual-uncased': 512,\n",
              " 'bert-base-uncased': 512,\n",
              " 'bert-large-cased': 512,\n",
              " 'bert-large-cased-whole-word-masking': 512,\n",
              " 'bert-large-cased-whole-word-masking-finetuned-squad': 512,\n",
              " 'bert-large-uncased': 512,\n",
              " 'bert-large-uncased-whole-word-masking': 512,\n",
              " 'bert-large-uncased-whole-word-masking-finetuned-squad': 512,\n",
              " 'wietsedv/bert-base-dutch-cased': 512}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "4TyrN1Ov0aqa",
        "outputId": "b0c392e9-a4a8-4ce3-b279-919efb6855a5"
      },
      "source": [
        "model['pooler'].dense()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bca509a94678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pooler'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'BertModel' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59ApHFR71vOI",
        "outputId": "e5a8a56d-354a-44f2-a572-4ce82b0bb0f8"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz_hyK7W1F0y",
        "outputId": "6022a389-666b-4d89-e9f1-c2054a76b76c"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "model.pooler['dense'] = nn.Linear(model.encoder.layer[11].output.dense.out_features)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertPooler(\n",
              "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (activation): Tanh()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJxrufpY1hzC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
