{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq-with attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sckvDxDNt8N9"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "import numpy as np\r\n",
        "import spacy\r\n",
        "import random\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crFX4oVjuZUQ",
        "outputId": "3030ed0b-f117-4861-ded9-e602faf1d233"
      },
      "source": [
        "!python -m spacy download  de_core_news_sm #en_core_web_sm\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907057 sha256=e789faa78a4ff8ec993b5242bd8a2d9b3626aa3b3e164666bada1dea12d18611\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v8257d94/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FOLMOkEuS9Z"
      },
      "source": [
        "spacy_ger = spacy.load('de_core_news_sm')\r\n",
        "spacy_eng = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3_Z_vNPuxen"
      },
      "source": [
        "def tokenizer_ger(text):\r\n",
        "    return [tok.text for tok in spacy_ger.tokenizer(text)]\r\n",
        "def tokenizer_eng(text):\r\n",
        "    return [tok.text for tok in spacy_eng.tokenizer(text)]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7e2cEYuvwQf"
      },
      "source": [
        "german = Field(tokenize=tokenizer_ger, lower=True, init_token='<sos>', eos_token='<eos>')\r\n",
        "english = Field(tokenize=tokenizer_eng, lower=True, init_token='<sos>', eos_token='<eos>')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjqiwMKnwRse"
      },
      "source": [
        "train_data, validation_data, test_data = Multi30k.splits(exts=('.de', '.en'),\r\n",
        "                                                         fields = (german, english))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stkkUmLVxyvD"
      },
      "source": [
        "german.build_vocab(train_data, max_size=10000, min_freq=2)\r\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H48nys1x8Ht"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, input_size, embedding_size, hidden_szie, num_layers, p):\r\n",
        "        super(Encoder, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(p)\r\n",
        "        self.embedding =  nn.Embedding(input_size, embedding_size)\r\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional = True, dropout=p)\r\n",
        "        self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\r\n",
        "        self.fc_cell = nn.Linear(hidden_size*2, hidden_size)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        embedding = self.dropout(self.embedding(x))\r\n",
        "        # shape of X is (seq leng, N), N is batch size\r\n",
        "        # embedding shape == (seq_length, N, embedding_size(lets say 300))\r\n",
        "\r\n",
        "        encoder_states, (hidden, cell) = self.rnn(embedding) \r\n",
        "        # output shape: (seq_length, Batch, hidden_size)\r\n",
        "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\r\n",
        "        cell  = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\r\n",
        "        # hidden shape: (2, Batch_size, hidden_szie)\r\n",
        "        return encoder_states, hidden, cell\r\n",
        "\r\n",
        "\r\n",
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\r\n",
        "        super(Decoder, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(p)\r\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\r\n",
        "        self.rnn = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, num_layers, dropout=p) # here we are sending the context vectore with the embedding size\r\n",
        "        #hidden size of encoder and decoder are the same\r\n",
        "\r\n",
        "\r\n",
        "        self.energy = nn.Linear(hidden_size*3, 1)\r\n",
        "        # Hidden state of the deocder of previous time step + encoder_states\r\n",
        "        self.softmax = nn.Softmax(dim=0)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\r\n",
        "\r\n",
        "    def forward(self, x, encoder_states, hidden, cell):\r\n",
        "        # shape of x is (N), we want  (1, N), in encoder we are sending a sentence, whereas in decoder we take a single word input from the predicted output and feed it as inoput\r\n",
        "        x = x.unsqueeze(0)\r\n",
        "        embedding = self.dropout(self.embedding(x))\r\n",
        "        # embedding shape is (1, N, embedding_size)\r\n",
        "\r\n",
        "        sequence_length = encoder_states.shape[0]\r\n",
        "        print(hidden.shape)\r\n",
        "        h_reshaped = hidden.repeat(sequence_length, 1, 1)# as the decoder hidden state is same for all outputs(enocder states) from the encoder\r\n",
        "        print(h_reshaped.shape)\r\n",
        "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\r\n",
        "        attention = self.softmax(energy)\r\n",
        "        # attention (seq_len, N, 1)\r\n",
        "\r\n",
        "        attention = attention.permute(1, 2, 0)\r\n",
        "        #(N,1 , seq_length)\r\n",
        "        #print(attention.shape)\r\n",
        "        encoder_states = encoder_states.permute(1, 0, 2)\r\n",
        "        # (N, seq_length, hidden_size*2)\r\n",
        "        #print(encoder_states.shape)\r\n",
        "        # (N, 1, hidden_size*2)\r\n",
        "        context_vector = torch.bmm(attention, encoder_states).permute(1, 0, 2)\r\n",
        "\r\n",
        "        rnn_input = torch.cat((context_vector, embedding), dim=2)\r\n",
        "\r\n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\r\n",
        "        # shape of output is 1, N, hidden_size\r\n",
        "        predictions = self.fc(output)\r\n",
        "        # shape of preds is (1, N, length of vocab)\r\n",
        "        predictions = predictions.squeeze(0)\r\n",
        "\r\n",
        "        return predictions, hidden, cell\r\n",
        "\r\n",
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder):\r\n",
        "        super(Seq2Seq, self).__init__()\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "\r\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\r\n",
        "        # teacher force ratio 50% of the time we send predicted outputs as inputs in the decoder and 50% of the time we send the actual values to the decoder inputs\r\n",
        "        batch_size = source.shape[1] #()\r\n",
        "        target_len = target.shape[0]\r\n",
        "        target_vocab_size = len(english.vocab)\r\n",
        "\r\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(\"cuda\")\r\n",
        "        encoder_states, hidden, cell = self.encoder(source)\r\n",
        "\r\n",
        "        x = target[0] # get start token\r\n",
        "        for t in range(1, target_len):\r\n",
        "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\r\n",
        "            # output has the final predcitons, outputs contain the whole sentences\r\n",
        "            outputs[t] = output\r\n",
        "\r\n",
        "            best_guess = output.argmax(1)\r\n",
        "\r\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\r\n",
        "\r\n",
        "        return outputs"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVGDv88ybS9e"
      },
      "source": [
        "Blog to understand the attention mechanism implementation better\r\n",
        "https://towardsdatascience.com/attaining-attention-in-deep-learning-a712f93bdb1e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFtcg1JqaQlq"
      },
      "source": [
        "# Training"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0TDHXqrayy7",
        "outputId": "6e0eb7a1-42c6-4695-a819-2d05d720eec9"
      },
      "source": [
        "num_epochs = 20\r\n",
        "learning_rate = 0.001\r\n",
        "batch_size = 64\r\n",
        "load_model = False\r\n",
        "device = torch.device('cuda')\r\n",
        "input_size_encoder = len(german.vocab)\r\n",
        "output_size_decoder = len(english.vocab)\r\n",
        "output_size = len(english.vocab)\r\n",
        "encoder_embedding_size = 300\r\n",
        "decoder_embedding_size = 300\r\n",
        "hidden_size = 1024 # used in seq2seq paper\r\n",
        "\r\n",
        "num_layers = 1\r\n",
        "enc_dropout = 0.5\r\n",
        "dec_dropout = 0.5\r\n",
        "\r\n",
        "# Tensorboard\r\n",
        "writer = SummaryWriter(f'runs/loss_plot')\r\n",
        "step = 0\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, validation_data, test_data), batch_size=batch_size, sort_within_batch=True, sort_key = lambda x: len(x.src), device=device)\r\n",
        "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout).to(device)\r\n",
        "decoder_net = Decoder(output_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout).to(device)\r\n",
        "\r\n",
        "model = Seq2Seq(encoder_net, decoder_net).to(device)\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\r\n",
        "\r\n",
        "pad_idx = english.vocab.stoi['<pad>']\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\r\n",
        "\r\n",
        "if load_model:\r\n",
        "    load_checkpoint(torch.load('my_checkpoint.pth.ptar'), model, optimizer)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "eADrsvRBkrRz",
        "outputId": "110ac29c-e616-4a85-a505-bd4ce972b709"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 19.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.19.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.94 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI3EZkvya0LO"
      },
      "source": [
        "import torch\r\n",
        "import spacy\r\n",
        "from torchtext.data.metrics import bleu_score\r\n",
        "import sys\r\n",
        "\r\n",
        "\r\n",
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\r\n",
        "    # print(sentence)\r\n",
        "\r\n",
        "    # sys.exit()\r\n",
        "\r\n",
        "    # Load german tokenizer\r\n",
        "    spacy_ger = spacy.load(\"de_core_news_sm\")\r\n",
        "\r\n",
        "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\r\n",
        "    if type(sentence) == str:\r\n",
        "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\r\n",
        "    else:\r\n",
        "        tokens = [token.lower() for token in sentence]\r\n",
        "\r\n",
        "    # print(tokens)\r\n",
        "\r\n",
        "    # sys.exit()\r\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\r\n",
        "    tokens.insert(0, german.init_token)\r\n",
        "    tokens.append(german.eos_token)\r\n",
        "\r\n",
        "    # Go through each german token and convert to an index\r\n",
        "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\r\n",
        "\r\n",
        "    # Convert to Tensor\r\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\r\n",
        "\r\n",
        "    # Build encoder hidden, cell state\r\n",
        "    with torch.no_grad():\r\n",
        "        o, hidden, cell = model.encoder(sentence_tensor)\r\n",
        "\r\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\r\n",
        "\r\n",
        "    for _ in range(max_length):\r\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            output, hidden, cell = model.decoder(previous_word, o, hidden, cell)\r\n",
        "            best_guess = output.argmax(1).item()\r\n",
        "\r\n",
        "        outputs.append(best_guess)\r\n",
        "\r\n",
        "        # Model predicts it's the end of the sentence\r\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\r\n",
        "            break\r\n",
        "\r\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\r\n",
        "\r\n",
        "    # remove start token\r\n",
        "    return translated_sentence[1:]\r\n",
        "\r\n",
        "\r\n",
        "def bleu(data, model, german, english, device):\r\n",
        "    targets = []\r\n",
        "    outputs = []\r\n",
        "\r\n",
        "    for example in data:\r\n",
        "        src = vars(example)[\"src\"]\r\n",
        "        trg = vars(example)[\"trg\"]\r\n",
        "\r\n",
        "        prediction = translate_sentence(model, src, german, english, device)\r\n",
        "        prediction = prediction[:-1]  # remove <eos> token\r\n",
        "\r\n",
        "        targets.append([trg])\r\n",
        "        outputs.append(prediction)\r\n",
        "\r\n",
        "    return bleu_score(outputs, targets)\r\n",
        "\r\n",
        "\r\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\r\n",
        "    print(\"=> Saving checkpoint\")\r\n",
        "    torch.save(state, filename)\r\n",
        "\r\n",
        "\r\n",
        "def load_checkpoint(checkpoint, model, optimizer):\r\n",
        "    print(\"=> Loading checkpoint\")\r\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\r\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6WHCEc_a0Ig",
        "outputId": "3932f7d8-ff91-495a-c0be-dc0cc7a6ca8f"
      },
      "source": [
        "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    print(f'Epoch [{epoch}/{num_epochs}]')\r\n",
        "\r\n",
        "    checkpoint = {'state_dict':model.state_dict(), 'optimizer':optimizer.state_dict()}\r\n",
        "    save_checkpoint(checkpoint)\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    translated_sentence = translate_sentence(\r\n",
        "        model, sentence, german, english, device, max_length=50\r\n",
        "    )\r\n",
        "\r\n",
        "    print(f\"Translated example sentence: \\n {translated_sentence}\")\r\n",
        "\r\n",
        "    model.train()\r\n",
        "    for batch_idx, batch in enumerate(train_iterator):\r\n",
        "        inp_data = batch.src.to(device)\r\n",
        "        target = batch.trg.to(device)\r\n",
        "        output = model(inp_data, target)\r\n",
        "\r\n",
        "        # output_shape: (trg_len, batch_size, output_dim )\r\n",
        "\r\n",
        "        output = output[1:].reshape(-1, output.shape[2])\r\n",
        "        target = target[1:].reshape(-1)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss = criterion(output, target)\r\n",
        "        loss.backward()\r\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1)\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        writer.add_scalar('Trainig Loss', loss, global_step=step)\r\n",
        "        step += 1"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', '.', 'horses', '.', '<eos>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [2/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [3/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'pulled', 'pulled', 'shore', 'by', 'shore', 'by', 'a', 'large', 'team', '.', '<eos>']\n",
            "Epoch [4/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [5/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [6/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'by', 'horses', '.', '<eos>']\n",
            "Epoch [7/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [8/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'horses', 'horses', 'horses', '.', '<eos>']\n",
            "Epoch [9/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', 'horses', '.', '<eos>']\n",
            "Epoch [10/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'is', 'horses', 'horses', 'horses', '.', '<eos>']\n",
            "Epoch [11/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [12/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [13/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [14/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['an', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [15/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [16/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [17/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [18/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
            "Epoch [19/20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMybfkNoa0Fc",
        "outputId": "6c54975a-a78c-4ce6-982f-bb48033d668e"
      },
      "source": [
        "score = bleu(test_data, model, german, english, device)\r\n",
        "print(f\"Bleu score {score*100:.2f}\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bleu score 22.07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90h6kBExaz7V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFJvy5lwaz4a"
      },
      "source": [
        "# Attention theory\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}